{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed06821d",
   "metadata": {},
   "source": [
    "<h1> Submitted by Ishan Ojha , CS-21 , Assignment - 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4192b",
   "metadata": {},
   "source": [
    "Forward propagation in RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05467e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20eb6722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  \n",
    "    return e_x / e_x.sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59629f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input sequence of indices\n",
    "x = [0, 1, 2, 1, 0]  \n",
    "input_dim = 3        # Size of input vocabulary\n",
    "hidden_dim = 5       # Size of the hidden layer\n",
    "output_dim = 3       # Size of the output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46575e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Initializing weights\n",
    "U = np.random.uniform(\n",
    "    -np.sqrt(1.0 / input_dim), np.sqrt(1.0 / input_dim),\n",
    "    (hidden_dim, input_dim)\n",
    ")\n",
    "W = np.random.uniform(\n",
    "    -np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),\n",
    "    (hidden_dim, hidden_dim)\n",
    ")\n",
    "V = np.random.uniform(\n",
    "    -np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim),\n",
    "    (output_dim, hidden_dim)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c23144de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of steps\n",
    "num_time_steps = len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d25fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hidden states\n",
    "hidden_state = np.zeros((num_time_steps + 1, hidden_dim))\n",
    "hidden_state[-1] = np.zeros(hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2206ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initializing output\n",
    "YHat = np.zeros((num_time_steps, output_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8899b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outputs (YHat):\n",
      "[[0.32998354 0.31817976 0.3518367 ]\n",
      " [0.33779166 0.34470571 0.31750263]\n",
      " [0.36640489 0.3560724  0.27752271]\n",
      " [0.35577164 0.35175996 0.2924684 ]\n",
      " [0.33885873 0.31881058 0.34233068]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#forward pass \n",
    "for t in np.arange(num_time_steps):\n",
    "    # Compute hidden state\n",
    "    # U[:, x[t]] selects the column of U corresponding to the input index x[t]\n",
    "    hidden_state[t] = np.tanh(U[:, x[t]] + W.dot(hidden_state[t - 1]))\n",
    "\n",
    "\n",
    "    YHat[t] = softmax(V.dot(hidden_state[t]))\n",
    "\n",
    "\n",
    "print(\"Predicted outputs (YHat):\")\n",
    "print(YHat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8063a",
   "metadata": {},
   "source": [
    "Generating song lyrics using RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9d4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c2029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing song data\n",
    "df = pd.read_csv('songdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "914d9dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83bc6d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of lyrics in dataset\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d43d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no of song lyrics\n",
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec9d862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist\n",
      "Donna Summer        191\n",
      "Gordon Lightfoot    189\n",
      "Bob Dylan           188\n",
      "George Strait       188\n",
      "Loretta Lynn        187\n",
      "Cher                187\n",
      "Alabama             187\n",
      "Reba Mcentire       187\n",
      "Chaka Khan          186\n",
      "Dean Martin         186\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # no of songs from each artist\n",
    "count =df['artist'].value_counts()[:10]\n",
    "print(count)\n",
    "print(\"\\n\")\n",
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "953013af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what co\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining all row of that column as data as the text variable \n",
    "data = ', '.join(df['text'])\n",
    "data[:369]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f279ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "char_to_ix['s']\n",
    "ix_to_char[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c3ea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabSize = 7\n",
    "char_index = 4\n",
    "\n",
    "np.eye(vocabSize)[char_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbec3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the one hot encoded vectors\n",
    "\n",
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c5ca2",
   "metadata": {},
   "source": [
    "Defining network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f40053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f39b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the number of units in the hidden layer:\n",
    "hidden_size = 100  \n",
    " \n",
    "#define the length of the input and output sequence:\n",
    "seq_length = 25  \n",
    "\n",
    "#define learning rate for gradient descent is as follows:\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#set the seed value:\n",
    "seed_value = 42\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb61c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=[None, vocab_size],dtype=tf.float32, name=\"inputs\")\n",
    "targets = Input(shape=[None, vocab_size], dtype=tf.float32, name=\"targets\")\n",
    "init_state =Input(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311006d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 1000\n",
    "hidden_size = 128\n",
    "seq_length = 10\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "# Simulate input: (seq_length, vocab_size)\n",
    "inputs = tf.random.uniform((seq_length, vocab_size))\n",
    "init_state = tf.zeros((hidden_size,))\n",
    "\n",
    "# Define parameters\n",
    "U = tf.Variable(initializer(shape=(vocab_size, hidden_size)), name=\"U\")\n",
    "W = tf.Variable(initializer(shape=(hidden_size, hidden_size)), name=\"W\")\n",
    "V = tf.Variable(initializer(shape=(hidden_size, vocab_size)), name=\"V\")\n",
    "bh = tf.Variable(tf.zeros([hidden_size]), name=\"bh\")\n",
    "by = tf.Variable(tf.zeros([vocab_size]), name=\"by\")\n",
    "\n",
    "# Forward pass\n",
    "h_t = init_state\n",
    "y_hat = []\n",
    "\n",
    "for t in range(seq_length):\n",
    "    x_t = inputs[t]  # Shape: (vocab_size,)\n",
    "    h_t = tf.tanh(tf.matmul(tf.expand_dims(x_t, 0), U) + tf.matmul(tf.expand_dims(h_t, 0), W) + bh)\n",
    "    y_t = tf.matmul(h_t, V) + by\n",
    "    y_hat.append(tf.squeeze(y_t))  # Remove batch dimension for consistency\n",
    "\n",
    "# Stack outputs: shape (seq_length, vocab_size)\n",
    "y_hat = tf.stack(y_hat, axis=0)\n",
    "\n",
    "print(\"Output shape:\", y_hat.shape)  # Should be (seq_length, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12393b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])  \n",
    "\n",
    "outputs = tf.concat(y_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a4c9f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nitro\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CrossEntropyLayer(tf.keras.layers.Layer):\n",
    "    def call(self, logits, labels):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "loss_layer = CrossEntropyLayer()\n",
    "loss = loss_layer(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9cdfded",
   "metadata": {},
   "outputs": [],
   "source": [
    "hprev = h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e1ccfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.953747\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_layer = CrossEntropyLayer()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "\n",
    "    for t in range(seq_length):\n",
    "        x_t = inputs[t]\n",
    "        h_t = tf.tanh(tf.matmul(tf.expand_dims(x_t, 0), U) +\n",
    "                      tf.matmul(tf.expand_dims(h_t, 0), W) + bh)\n",
    "        y_t = tf.matmul(h_t, V) + by\n",
    "        y_hat.append(tf.squeeze(y_t))\n",
    "\n",
    "    # Stack outputs: (seq_length, vocab_size)\n",
    "    outputs = tf.stack(y_hat, axis=0)\n",
    "\n",
    "    # Create one-hot encoded targets for use with softmax_cross_entropy_with_logits\n",
    "    target_indices = tf.random.uniform((seq_length,), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "    targets = tf.one_hot(target_indices, depth=vocab_size)\n",
    "\n",
    "    # Compute loss using your custom layer\n",
    "    loss = loss_layer(outputs, targets)\n",
    "\n",
    "# Clip gradients and apply\n",
    "model_variables = [U, W, V, bh, by]\n",
    "gradients = tape.gradient(loss, model_variables)\n",
    "clipped_gradients = [tf.clip_by_value(g, -5.0, 5.0) for g in gradients]\n",
    "optimizer.apply_gradients(zip(clipped_gradients, model_variables))\n",
    "\n",
    "print(\"Loss:\", loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fd526f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution is enabled: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Eager execution is enabled:\", tf.executing_eagerly())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12468ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
